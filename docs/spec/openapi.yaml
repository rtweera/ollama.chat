openapi: 3.1.0
info:
  title: Ollama Chat Completion API
  description: API for generating chat completions using the Ollama server.
  version: 0.1.0
servers:
  - url: http://localhost:11434/api
    description: Local Ollama server
paths:
  /chat:
    post:
      summary: Generate a chat completion
      description: |
        Generate a chat completion for a given list of messages with a provided model. This endpoint supports 
        streaming (default) or non-streaming responses. In streaming mode, it returns a series of JSON objects; 
        in non-streaming mode, it returns a single JSON object with the full response and metadata.
      operationId: generateChatCompletion
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
            examples:
              basicStreaming:
                summary: Basic streaming request
                value:
                  model: "llama3.2"
                  messages:
                    - role: "user"
                      content: "why is the sky blue?"
              nonStreaming:
                summary: Non-streaming request
                value:
                  model: "llama3.2"
                  messages:
                    - role: "user"
                      content: "why is the sky blue?"
                  stream: false
              withSystemMessage:
                summary: Request with system message
                value:
                  model: "llama3.2"
                  messages:
                    - role: "system"
                      content: "You are a helpful assistant."
                    - role: "user"
                      content: "why is the sky blue?"
                  stream: false
              withImages:
                summary: Request with images (multimodal)
                value:
                  model: "llava"
                  messages:
                    - role: "user"
                      content: "Whatâ€™s in this image?"
                      images:
                        - "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
                  stream: false
              withOptions:
                summary: Request with custom options
                value:
                  model: "llama3.2"
                  messages:
                    - role: "user"
                      content: "why is the sky blue?"
                  stream: false
                  options:
                    temperature: 0.8
                    top_p: 0.9
                    num_ctx: 2048
              jsonFormat:
                summary: JSON formatted response
                value:
                  model: "llama3.2"
                  messages:
                    - role: "user"
                      content: "Provide a JSON object with sky color explanations."
                  format: "json"
                  stream: false
      responses:
        '200':
          description: |
            Success. Returns a stream of JSON objects if `stream` is true (default), or a single JSON object if `stream` is false.
          content:
            application/json:
              schema:
                oneOf:
                  - $ref: '#/components/schemas/ChatStreamResponse'
                  - $ref: '#/components/schemas/ChatSingleResponse'
              examples:
                streamingPartial:
                  summary: Streaming response (partial)
                  value:
                    model: "llama3.2"
                    created_at: "2024-03-19T12:00:00Z"
                    message:
                      role: "assistant"
                      content: "The sky appears"
                    done: false
                streamingFinal:
                  summary: Streaming final response
                  value:
                    model: "llama3.2"
                    created_at: "2024-03-19T12:00:01Z"
                    message:
                      role: "assistant"
                      content: ""
                    done: true
                    done_reason: "stop"
                    total_duration: 5194830000
                    load_duration: 4490000
                    prompt_eval_count: 23
                    prompt_eval_duration: 189000000
                    eval_count: 102
                    eval_duration: 4996000000
                nonStreaming:
                  summary: Non-streaming response
                  value:
                    model: "llama3.2"
                    created_at: "2024-03-19T12:00:01Z"
                    message:
                      role: "assistant"
                      content: "The sky appears blue due to Rayleigh scattering, where shorter wavelengths of light (blue) scatter more than longer ones (red) in the atmosphere."
                    done: true
                    done_reason: "stop"
                    total_duration: 5194830000
                    load_duration: 4490000
                    prompt_eval_count: 23
                    prompt_eval_duration: 189000000
                    eval_count: 102
                    eval_duration: 4996000000
                withImages:
                  summary: Response with images
                  value:
                    model: "llava"
                    created_at: "2024-03-19T12:00:01Z"
                    message:
                      role: "assistant"
                      content: "The image shows a small black square."
                    done: true
                    total_duration: 2938432250
                    load_duration: 2559292
                    prompt_eval_count: 1
                    prompt_eval_duration: 2195557000
                    eval_count: 44
                    eval_duration: 736432000
                jsonFormat:
                  summary: JSON formatted response
                  value:
                    model: "llama3.2"
                    created_at: "2024-03-19T12:00:01Z"
                    message:
                      role: "assistant"
                      content: "{\"reason\": \"Rayleigh scattering\", \"color\": \"blue\"}"
                    done: true
                    total_duration: 4648158584
                    load_duration: 4071084
                    prompt_eval_count: 36
                    prompt_eval_duration: 439038000
                    eval_count: 180
                    eval_duration: 4196918000
        '400':
          description: Bad Request. Returned when the request is malformed or invalid (e.g., missing required fields).
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              example:
                error: "invalid request: 'model' field is required"
        '404':
          description: Not Found. Returned when the specified model does not exist.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              example:
                error: "model 'llama3.2:3b' not found"
        '500':
          description: Internal Server Error. Returned when an unexpected error occurs on the server.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              example:
                error: "internal server error: failed to process chat request"
components:
  schemas:
    ChatRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: The model name in `model:tag` format (e.g., `llama3:70b`). Tag defaults to `latest` if omitted.
          example: "llama3.2"
        messages:
          type: array
          items:
            $ref: '#/components/schemas/Message'
          description: List of messages in the chat history, including roles and content.
          minItems: 0
        format:
          type: string
          enum: ["json"]
          description: If set to `"json"`, the response content will be formatted as a JSON string.
          example: "json"
        options:
          $ref: '#/components/schemas/ChatOptions'
        stream:
          type: boolean
          description: If `false`, returns a single response object instead of a stream. Defaults to `true`.
          default: true
        keep_alive:
          oneOf:
            - type: string
            - type: integer
          description: Duration the model stays loaded in memory after the request (e.g., `5m`). In nanoseconds if numeric.
          default: "5m"
    Message:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant", "tool"]
          description: The role of the message sender.
          example: "user"
        content:
          type: string
          description: The text content of the message.
          example: "why is the sky blue?"
        images:
          type: array
          items:
            type: string
            format: byte
          description: List of base64-encoded images (for multimodal models like `llava`).
    ChatOptions:
      type: object
      description: Additional model parameters for chat completion.
      properties:
        num_keep:
          type: integer
          description: Number of tokens to keep.
        seed:
          type: integer
          description: Seed for reproducible outputs.
        num_predict:
          type: integer
          description: Maximum number of tokens to predict.
        top_k:
          type: integer
          description: Top-k sampling parameter.
        top_p:
          type: number
          description: Top-p (nucleus) sampling parameter.
        min_p:
          type: number
          description: Minimum probability threshold.
        typical_p:
          type: number
          description: Typical probability parameter.
        repeat_last_n:
          type: integer
          description: Number of last tokens to consider for repetition penalty.
        temperature:
          type: number
          description: Sampling temperature.
        repeat_penalty:
          type: number
          description: Penalty for repeated tokens.
        presence_penalty:
          type: number
          description: Penalty for presence of certain tokens.
        frequency_penalty:
          type: number
          description: Penalty based on token frequency.
        mirostat:
          type: integer
          description: Mirostat sampling mode.
        mirostat_tau:
          type: number
          description: Mirostat tau parameter.
        mirostat_eta:
          type: number
          description: Mirostat eta parameter.
        penalize_newline:
          type: boolean
          description: Whether to penalize newlines.
        stop:
          type: array
          items:
            type: string
          description: List of stop sequences.
        numa:
          type: boolean
          description: Enable NUMA optimizations.
        num_ctx:
          type: integer
          description: Context window size.
        num_batch:
          type: integer
          description: Batch size for processing.
        num_gpu:
          type: integer
          description: Number of GPUs to use.
        main_gpu:
          type: integer
          description: Main GPU index.
        low_vram:
          type: boolean
          description: Optimize for low VRAM usage.
        vocab_only:
          type: boolean
          description: Load only the vocabulary.
        use_mmap:
          type: boolean
          description: Use memory-mapped files.
        use_mlock:
          type: boolean
          description: Lock memory to prevent swapping.
        num_thread:
          type: integer
          description: Number of threads to use.
    ChatStreamResponse:
      type: object
      description: A single object in the stream of responses when `stream` is `true`.
      required:
        - model
        - created_at
        - message
        - done
      properties:
        model:
          type: string
          description: The model name used for generation.
          example: "llama3.2"
        created_at:
          type: string
          format: datetime
          description: Timestamp of response creation.
          example: "2024-03-19T12:00:00Z"
        message:
          $ref: '#/components/schemas/Message'
          description: The assistant's response message (content may be empty in the final response).
        done:
          type: boolean
          description: Indicates if this is the final response in the stream.
          example: false
        done_reason:
          type: string
          description: Reason for completion (e.g., "stop"), present only in the final response.
          example: "stop"
        total_duration:
          type: integer
          format: int64
          description: Total time spent generating the response in nanoseconds (final response only).
          example: 5194830000
        load_duration:
          type: integer
          format: int64
          description: Time spent loading the model in nanoseconds (final response only).
          example: 4490000
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt (final response only).
          example: 23
        prompt_eval_duration:
          type: integer
          format: int64
          description: Time spent evaluating the prompt in nanoseconds (final response only).
          example: 189000000
        eval_count:
          type: integer
          description: Number of tokens in the response (final response only).
          example: 102
        eval_duration:
          type: integer
          format: int64
          description: Time spent generating the response in nanoseconds (final response only).
          example: 4996000000
    ChatSingleResponse:
      type: object
      description: Response when `stream` is `false`.
      required:
        - model
        - created_at
        - message
        - done
      properties:
        model:
          type: string
          description: The model name used for generation.
          example: "llama3.2"
        created_at:
          type: string
          format: datetime
          description: Timestamp of response creation.
          example: "2024-03-19T12:00:01Z"
        message:
          $ref: '#/components/schemas/Message'
          description: The assistant's complete response message.
        done:
          type: boolean
          description: Always `true` for a single response.
          example: true
        done_reason:
          type: string
          description: Reason for completion (e.g., "stop").
          example: "stop"
        total_duration:
          type: integer
          format: int64
          description: Total time spent generating the response in nanoseconds.
          example: 5194830000
        load_duration:
          type: integer
          format: int64
          description: Time spent loading the model in nanoseconds.
          example: 4490000
        prompt_eval_count:
          type: integer
          description: Number of tokens in the prompt.
          example: 23
        prompt_eval_duration:
          type: integer
          format: int64
          description: Time spent evaluating the prompt in nanoseconds.
          example: 189000000
        eval_count:
          type: integer
          description: Number of tokens in the response.
          example: 102
        eval_duration:
          type: integer
          format: int64
          description: Time spent generating the response in nanoseconds.
          example: 4996000000
    ErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: A human-readable error message.
          example: "model 'llama3.2:3b' not found"